{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Project     : mrs - multi-turn response selection\n",
    "# Created By  : Eungis\n",
    "# Team        : AI Engineering\n",
    "# Created Date: 2023-11-30\n",
    "# Updated Date: 2024-02-20\n",
    "# Purpose     : Make data_loader for loading data\n",
    "# version     : 0.0.1\n",
    "# ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "from importlib import reload  # Not needed in Python 2\n",
    "\n",
    "reload(logging)\n",
    "logging.basicConfig(\n",
    "    format=\"%(message)s\",\n",
    "    level=logging.DEBUG,\n",
    ")\n",
    "\n",
    "DATA_ROOT = \"../data/\"\n",
    "LOGGER = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Session:\n",
    "    conv: List[str]\n",
    "    \"\"\"Conversation: List of utterences\"\"\"\n",
    "\n",
    "\n",
    "class SessionBuilder:\n",
    "    def __init__(self, style: str):\n",
    "        self.style = style\n",
    "        self.logger = self._set_logger()\n",
    "\n",
    "    def _set_logger(self):\n",
    "        logger = logging.getLogger(__name__)\n",
    "        return logger\n",
    "\n",
    "    def build_sessions(self, data_path: str) -> List[List[str]]:\n",
    "        # data to load must be separated with `tab`\n",
    "        data = pd.read_csv(data_path, sep=\"\\t\")\n",
    "        styles = data.columns.tolist()\n",
    "        if self.style not in styles:\n",
    "            raise ValueError(\n",
    "                f\"Unsupported style. Style must be one of {styles}.\\nInput: {self.style}\"\n",
    "            )\n",
    "\n",
    "        # use specified style conversational data\n",
    "        data = data[[self.style]]\n",
    "        data[\"group\"] = data[self.style].isnull().cumsum()\n",
    "        n_sessions = data[\"group\"].iat[-1] + 1\n",
    "        self.logger.debug(f\"Number of sessions: {n_sessions}\")\n",
    "\n",
    "        # split data into sessions\n",
    "        sessions: List[Session] = []\n",
    "        groups = data.groupby(\"group\", as_index=False, group_keys=False)\n",
    "\n",
    "        for i, group in groups:\n",
    "            session = group.dropna()[self.style].tolist()\n",
    "            sessions += [Session(conv=session)]\n",
    "\n",
    "        assert n_sessions == len(sessions)\n",
    "        return sessions\n",
    "\n",
    "    def build_short_sessions(\n",
    "        self, sessions: List[Session], ctx_len: int = 4\n",
    "    ) -> List[Session]:\n",
    "        short_sessions = []\n",
    "        for session in sessions:\n",
    "            for i in range(len(session.conv) - ctx_len + 1):\n",
    "                short_sessions.append(Session(conv=session.conv[i : i + ctx_len]))\n",
    "        return short_sessions\n",
    "\n",
    "    def get_utterances(self, sessions: List[Session]):\n",
    "        all_utts = set()\n",
    "        for session in sessions:\n",
    "            for utt in session.conv:\n",
    "                all_utts.add(utt)\n",
    "        return list(all_utts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of sessions: 236\n",
      "Starting new HTTPS connection (1): huggingface.co:443\n",
      "https://huggingface.co:443 \"HEAD /klue/roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "Special tokens: {'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "EOS token & SEP token: [SEP] / [SEP]\n",
      "Special tokens map: {'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '<SEP>', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "[SEP]: 2\n",
      "<SEP>: 32000\n",
      "[MASK]: 4\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "builder = SessionBuilder(style=\"formal\")\n",
    "sessions = builder.build_sessions(data_path=DATA_ROOT + \"smilestyle_dataset.tsv\")\n",
    "short_sessions = builder.build_short_sessions(sessions, ctx_len=4)\n",
    "utts = builder.get_utterances(sessions)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")\n",
    "LOGGER.debug(f\"Special tokens: {tokenizer.special_tokens_map}\")\n",
    "LOGGER.debug(f\"EOS token & SEP token: {tokenizer.eos_token} / {tokenizer.sep_token}\")\n",
    "special_tokens = {\"sep_token\": \"<SEP>\"}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "LOGGER.info(\n",
    "    f\"\"\"Special tokens map: {tokenizer.special_tokens_map}\n",
    "{tokenizer.eos_token}: {tokenizer.eos_token_id}\n",
    "{tokenizer.sep_token}: {tokenizer.sep_token_id}\n",
    "{tokenizer.mask_token}: {tokenizer.mask_token_id}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"klue/roberta-base\"\n",
    "DATA_ROOT = \"../data/\"\n",
    "DATA_PATH = DATA_ROOT + \"smilestyle_dataset.tsv\"\n",
    "\n",
    "\n",
    "class PostDataset(Dataset):\n",
    "    def __init__(self, builder: SessionBuilder):\n",
    "        # set logger\n",
    "        self.logger = self._set_logger()\n",
    "\n",
    "        # set tokenizer\n",
    "        self.tokenizer = self._set_tokenizer()\n",
    "\n",
    "        # build sessions\n",
    "        self.sessions = builder.build_sessions(data_path=DATA_PATH)\n",
    "        self.short_sessions = builder.build_short_sessions(self.sessions, ctx_len=4)\n",
    "        self.utts = builder.get_utterances(self.sessions)\n",
    "\n",
    "    def _set_logger(self):\n",
    "        logger = logging.getLogger(__name__)\n",
    "        return logger\n",
    "\n",
    "    def _set_tokenizer(self):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        special_tokens = {\"sep_token\": \"<SEP>\"}\n",
    "        tokenizer.add_special_tokens(special_tokens)\n",
    "        return tokenizer\n",
    "\n",
    "    def _mask_tokens(self, tokens: List, ratio: float = 0.15) -> List:\n",
    "        tokens = np.array(tokens)\n",
    "        n_mask = int(len(tokens) * ratio)\n",
    "        mask_pos = random.sample(range(len(tokens)), n_mask)\n",
    "\n",
    "        # fancy indexing\n",
    "        tokens[mask_pos] = self.tokenizer.mask_token_id\n",
    "        tokens = tokens.tolist()\n",
    "        return tokens\n",
    "\n",
    "    def _get_mask_positions(self, tokens: List) -> List:\n",
    "        tokens = np.array(tokens)\n",
    "        mask_positions = np.where(tokens == self.tokenizer.mask_token_id)[0].tolist()\n",
    "        return mask_positions\n",
    "\n",
    "    def construct_mlm_inputs(self, short_session: Session) -> Dict[str, list]:\n",
    "        corrupt_tokens, output_tokens = [], []\n",
    "\n",
    "        for i, utt in enumerate(short_session.conv):\n",
    "            tokens = self.tokenizer.encode(utt, add_special_tokens=False)\n",
    "            masked_tokens = self._mask_tokens(tokens)\n",
    "\n",
    "            if i == len(short_session.conv) - 1:\n",
    "                output_tokens.extend(tokens)\n",
    "                corrupt_tokens.extend(masked_tokens)\n",
    "            else:\n",
    "                output_tokens.extend(tokens + [self.tokenizer.sep_token_id])\n",
    "                corrupt_tokens.extend(masked_tokens + [self.tokenizer.sep_token_id])\n",
    "\n",
    "        corrupt_mask_positions = self._get_mask_positions(corrupt_tokens)\n",
    "        return_value = {\n",
    "            \"output_tokens\": output_tokens,\n",
    "            \"corrupt_tokens\": corrupt_tokens,\n",
    "            \"corrupt_mask_positions\": corrupt_mask_positions,\n",
    "        }\n",
    "\n",
    "        return return_value\n",
    "\n",
    "    def construct_urc_inputs(self, short_session: Session) -> Dict[str, List]:\n",
    "        urc_tokens, ctx_utts = [], []\n",
    "\n",
    "        for i in range(len(short_session.conv)):\n",
    "            utt = short_session.conv[i]\n",
    "            tokens = self.tokenizer.encode(utt, add_special_tokens=False)\n",
    "\n",
    "            if i == len(short_session.conv) - 1:\n",
    "                urc_tokens += [self.tokenizer.eos_token_id]\n",
    "                positive_tokens = [self.tokenizer.cls_token_id] + urc_tokens + tokens\n",
    "\n",
    "                while True:\n",
    "                    random_neg_response = random.choice(self.utts)\n",
    "                    if random_neg_response not in ctx_utts:\n",
    "                        break\n",
    "                random_neg_response_token = self.tokenizer.encode(\n",
    "                    random_neg_response, add_special_tokens=False\n",
    "                )\n",
    "                random_tokens = (\n",
    "                    [self.tokenizer.cls_token_id]\n",
    "                    + urc_tokens\n",
    "                    + random_neg_response_token\n",
    "                )\n",
    "                ctx_neg_response = random.choice(ctx_utts)\n",
    "                ctx_neg_response_token = self.tokenizer.encode(\n",
    "                    ctx_neg_response, add_special_tokens=False\n",
    "                )\n",
    "                ctx_neg_tokens = (\n",
    "                    [self.tokenizer.cls_token_id] + urc_tokens + ctx_neg_response_token\n",
    "                )\n",
    "            else:\n",
    "                urc_tokens += tokens + [self.tokenizer.sep_token_id]\n",
    "\n",
    "            ctx_utts.append(utt)\n",
    "\n",
    "        return_value = {\n",
    "            \"positive_tokens\": positive_tokens,\n",
    "            \"random_negative_tokens\": random_tokens,\n",
    "            \"context_negative_tokens\": ctx_neg_tokens,\n",
    "            \"urc_labels\": [0, 1, 2],\n",
    "        }\n",
    "        return return_value\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.short_sessions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # ---- input data for MLM ---- #\n",
    "        short_session = self.short_sessions[idx]\n",
    "        mlm_input = self.construct_mlm_inputs(short_session)\n",
    "\n",
    "        # ---- intput data for utterance relevance classification ---- #\n",
    "        urc_input = self.construct_urc_inputs(short_session)\n",
    "\n",
    "        return_value = dict()\n",
    "        return_value[\"mlm_input\"] = mlm_input\n",
    "        return_value[\"urc_input\"] = urc_input\n",
    "\n",
    "        return return_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co:443 \"HEAD /klue/roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "Number of sessions: 236\n"
     ]
    }
   ],
   "source": [
    "builder = SessionBuilder(style=\"formal\")\n",
    "post_dataset = PostDataset(builder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요. 저는 고양이 6마리 키워요. <SEP> 고양이를 6마리나요? 키우는거 안 힘드세요? <SEP> 제가 워낙 고양이를 좋아해서 크게 힘들진 않아요. <SEP> 가장 나이가 많은 고양이가 어떻게 돼요?\n",
      "안녕하 [MASK]. 저는 고양이 6마리 키워요. <SEP> 고양이를 6마리나요 [MASK] 키우는거 안 힘드세요 [MASK] <SEP> 제가 워낙 [MASK]를 좋아해서 [MASK]게 힘들진 않아요. <SEP> 가장 나이가 많은 고양이가 어떻게 돼요 [MASK]\n",
      "[2, 19, 26, 31, 34, 52]\n",
      "[CLS] 안녕하세요. 저는 고양이 6마리 키워요. <SEP> 고양이를 6마리나요? 키우는거 안 힘드세요? <SEP> 제가 워낙 고양이를 좋아해서 크게 힘들진 않아요. <SEP> [SEP] 가장 나이가 많은 고양이가 어떻게 돼요?\n",
      "[CLS] 안녕하세요. 저는 고양이 6마리 키워요. <SEP> 고양이를 6마리나요? 키우는거 안 힘드세요? <SEP> 제가 워낙 고양이를 좋아해서 크게 힘들진 않아요. <SEP> [SEP] 저는 캘리포니아에서 왔어요. 어떤 일을 하시나요?\n",
      "[CLS] 안녕하세요. 저는 고양이 6마리 키워요. <SEP> 고양이를 6마리나요? 키우는거 안 힘드세요? <SEP> 제가 워낙 고양이를 좋아해서 크게 힘들진 않아요. <SEP> [SEP] 고양이를 6마리나요? 키우는거 안 힘드세요?\n"
     ]
    }
   ],
   "source": [
    "sample = post_dataset[0]\n",
    "print(post_dataset.tokenizer.decode(sample[\"mlm_input\"][\"output_tokens\"]))\n",
    "print(post_dataset.tokenizer.decode(sample[\"mlm_input\"][\"corrupt_tokens\"]))\n",
    "print(sample[\"mlm_input\"][\"corrupt_mask_positions\"])\n",
    "print(post_dataset.tokenizer.decode(sample[\"urc_input\"][\"positive_tokens\"]))\n",
    "print(post_dataset.tokenizer.decode(sample[\"urc_input\"][\"random_negative_tokens\"]))\n",
    "print(post_dataset.tokenizer.decode(sample[\"urc_input\"][\"context_negative_tokens\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "class PostDatasetCollator:\n",
    "    def __init__(self, pad_idx: int, max_length: int):\n",
    "        self.pad_idx = pad_idx\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, batch: List[dict]):\n",
    "        # |batch| = [\n",
    "        # {\n",
    "        # 'mlm_input': {\n",
    "        # 'output_tokens': list(),\n",
    "        # 'corrupt_tokens': list(),\n",
    "        # 'corrupt_mask_positions': list()\n",
    "        # },\n",
    "        # 'urc_input': {\n",
    "        # 'positive_tokens': list(),\n",
    "        # 'random_negative_tokens': list(),\n",
    "        # 'context_negative_tokens': list(),\n",
    "        # 'urc_labels': list()\n",
    "        # }\n",
    "        # }, ...\n",
    "        # ]\n",
    "\n",
    "        # initialize batch output bags\n",
    "        mlm_output_tokens_inputs, mlm_corrupt_tokens_inputs = [], []\n",
    "        urc_inputs = []\n",
    "        mlm_corrupt_mask_positions, urc_labels = [], []\n",
    "\n",
    "        for sample in batch:\n",
    "            mlm_input, urc_input = sample[\"mlm_input\"], sample[\"urc_input\"]\n",
    "\n",
    "            mlm_output_tokens_inputs.append(\n",
    "                torch.tensor(mlm_input[\"output_tokens\"][: self.max_length])\n",
    "            )\n",
    "            mlm_corrupt_tokens_inputs.append(\n",
    "                torch.tensor(mlm_input[\"corrupt_tokens\"][: self.max_length])\n",
    "            )\n",
    "            mlm_corrupt_mask_positions.append(\n",
    "                torch.tensor(mlm_input[\"corrupt_mask_positions\"])\n",
    "            )\n",
    "\n",
    "            urc_inputs.append(\n",
    "                torch.tensor(urc_input[\"positive_tokens\"][: self.max_length])\n",
    "            )\n",
    "            urc_inputs.append(\n",
    "                torch.tensor(urc_input[\"random_negative_tokens\"][: self.max_length])\n",
    "            )\n",
    "            urc_inputs.append(\n",
    "                torch.tensor(urc_input[\"context_negative_tokens\"][: self.max_length])\n",
    "            )\n",
    "            urc_labels.append(torch.tensor(urc_input[\"urc_labels\"]))\n",
    "\n",
    "        # pad sequence\n",
    "        mlm_output_tokens_inputs = pad_sequence(\n",
    "            mlm_output_tokens_inputs, batch_first=True, padding_value=self.pad_idx\n",
    "        )\n",
    "        mlm_corrupt_tokens_inputs = pad_sequence(\n",
    "            mlm_corrupt_tokens_inputs, batch_first=True, padding_value=self.pad_idx\n",
    "        )\n",
    "\n",
    "        urc_inputs = pad_sequence(\n",
    "            urc_inputs, batch_first=True, padding_value=self.pad_idx\n",
    "        )\n",
    "\n",
    "        # get attention masking positions\n",
    "        mlm_attentions = (mlm_output_tokens_inputs != self.pad_idx).long()\n",
    "        urc_attentions = (urc_inputs != self.pad_idx).long()\n",
    "\n",
    "        return_value = {\n",
    "            \"mlm_inputs\": {\n",
    "                \"output_tokens\": mlm_output_tokens_inputs,\n",
    "                \"corrupt_tokens\": mlm_corrupt_tokens_inputs,\n",
    "                \"mask_positions\": mlm_corrupt_mask_positions,\n",
    "                \"attention_masks\": mlm_attentions,\n",
    "            },\n",
    "            \"urc_inputs\": {\n",
    "                \"input_tokens\": urc_inputs,\n",
    "                \"labels\": urc_labels,\n",
    "                \"attention_masks\": urc_attentions,\n",
    "            },\n",
    "        }\n",
    "        return return_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "post_dataloader = DataLoader(\n",
    "    post_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=PostDatasetCollator(\n",
    "        pad_idx=post_dataset.tokenizer.pad_token_id, max_length=99999\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = next(iter(post_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_mlm_inputs = sample_batch[\"mlm_inputs\"]\n",
    "sample_urc_inputs = sample_batch[\"urc_inputs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 86]), torch.Size([2, 86]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_mlm_inputs[\"output_tokens\"].shape, sample_mlm_inputs[\"corrupt_tokens\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('로봇이 능동적으로 사람을 속이려고 할 수 있다는 말씀이신가요? <SEP> 예를 들면 두 명의 사람이 있고, 한명을 살리기 위해 한명을 죽여야 한다면, 로봇은 사람에게 해를 끼칠 수 있지 않을까요? <SEP> 제 생각에는 가만히 있을 것 같아요. <SEP> 제 생각에는 가만히 있지 않고 해를 끼칠수도 있다고 봅니다.',\n",
       " '로봇이 능동적으로 사람 [MASK] 속이 [MASK] 할 수 있다는 말씀이신가요? <SEP> [MASK]를 들면 두 명의 [MASK]이 있고, 한명을 살리기 위해 [MASK] [MASK] 죽여야 한다면, 로봇은 사람에게 해를 끼칠 [MASK] 있지 않을까요? <SEP> 제 생각에는 가만히 [MASK]을 것 같아요. <SEP> [MASK] 생각에는 가만히 있지 않고 해를 [MASK]수도 있다고 봅니다.')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_dataset.tokenizer.decode(\n",
    "    sample_mlm_inputs[\"output_tokens\"][0, :]\n",
    "), post_dataset.tokenizer.decode(sample_mlm_inputs[\"corrupt_tokens\"][0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('네, 자이언트 팬입니다. <SEP> 자이언트 팀은 어디 팀인가요? <SEP> 신생 팀인데, 아리조나 대학교 팀입니다. <SEP> 아 그렇군요, 저는 캘리포니아 호크를 응원하고 있습니다. [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       " '네, 자이언 [MASK] 팬입니다. <SEP> 자이언트 [MASK]은 어디 팀인가요? <SEP> 신생 팀인데, 아리조나 대학교 [MASK]입니다. <SEP> 아 그렇군 [MASK], 저 [MASK] 캘리포니아 호크를 응원하고 있습니다. [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_dataset.tokenizer.decode(\n",
    "    sample_mlm_inputs[\"output_tokens\"][1, :]\n",
    "), post_dataset.tokenizer.decode(sample_mlm_inputs[\"corrupt_tokens\"][1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([ 5,  7, 18, 24, 34, 35, 48, 61, 69, 80]),\n",
       "  tensor([ 3, 10, 27, 34, 37])],\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_mlm_inputs[\"mask_positions\"], sample_mlm_inputs[\"attention_masks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[CLS] 로봇이 능동적으로 사람을 속이려고 할 수 있다는 말씀이신가요? <SEP> 예를 들면 두 명의 사람이 있고, 한명을 살리기 위해 한명을 죽여야 한다면, 로봇은 사람에게 해를 끼칠 수 있지 않을까요? <SEP> 제 생각에는 가만히 있을 것 같아요. <SEP> [SEP] 제 생각에는 가만히 있지 않고 해를 끼칠수도 있다고 봅니다. [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       " '[CLS] 로봇이 능동적으로 사람을 속이려고 할 수 있다는 말씀이신가요? <SEP> 예를 들면 두 명의 사람이 있고, 한명을 살리기 위해 한명을 죽여야 한다면, 로봇은 사람에게 해를 끼칠 수 있지 않을까요? <SEP> 제 생각에는 가만히 있을 것 같아요. <SEP> [SEP] 어떤 의문이요? [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       " '[CLS] 로봇이 능동적으로 사람을 속이려고 할 수 있다는 말씀이신가요? <SEP> 예를 들면 두 명의 사람이 있고, 한명을 살리기 위해 한명을 죽여야 한다면, 로봇은 사람에게 해를 끼칠 수 있지 않을까요? <SEP> 제 생각에는 가만히 있을 것 같아요. <SEP> [SEP] 예를 들면 두 명의 사람이 있고, 한명을 살리기 위해 한명을 죽여야 한다면, 로봇은 사람에게 해를 끼칠 수 있지 않을까요?',\n",
       " '[CLS] 네, 자이언트 팬입니다. <SEP> 자이언트 팀은 어디 팀인가요? <SEP> 신생 팀인데, 아리조나 대학교 팀입니다. <SEP> [SEP] 아 그렇군요, 저는 캘리포니아 호크를 응원하고 있습니다. [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       " '[CLS] 네, 자이언트 팬입니다. <SEP> 자이언트 팀은 어디 팀인가요? <SEP> 신생 팀인데, 아리조나 대학교 팀입니다. <SEP> [SEP] 소와 돼지도요? [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       " '[CLS] 네, 자이언트 팬입니다. <SEP> 자이언트 팀은 어디 팀인가요? <SEP> 신생 팀인데, 아리조나 대학교 팀입니다. <SEP> [SEP] 신생 팀인데, 아리조나 대학교 팀입니다. [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_dataset.tokenizer.decode(\n",
    "    sample_urc_inputs[\"input_tokens\"][0]\n",
    "), post_dataset.tokenizer.decode(\n",
    "    sample_urc_inputs[\"input_tokens\"][1]\n",
    "), post_dataset.tokenizer.decode(\n",
    "    sample_urc_inputs[\"input_tokens\"][2]\n",
    "), post_dataset.tokenizer.decode(\n",
    "    sample_urc_inputs[\"input_tokens\"][3]\n",
    "), post_dataset.tokenizer.decode(\n",
    "    sample_urc_inputs[\"input_tokens\"][4]\n",
    "), post_dataset.tokenizer.decode(\n",
    "    sample_urc_inputs[\"input_tokens\"][5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 1, 2]), tensor([0, 1, 2])]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_urc_inputs[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_urc_inputs[\"attention_masks\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
